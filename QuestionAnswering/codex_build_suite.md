# ðŸ§© **PHASE 0 â€” MASTER OVERVIEW PROMPT (Run first in Codex)**

```markdown
# PROJECT OVERVIEW â€” DO NOT GENERATE CODE YET

You are Codex. Your task is to generate a complete Python project, but we start with THIS overview prompt.  
**In this prompt do not generate any Python code. Only output the PROJECT STRUCTURE and MODULE DESCRIPTIONS.**

---

# ðŸ§  PROJECT DESCRIPTION

We are building a **local, multilingual Question Answering System** using:

- Python
- NiceGUI (UI)
- Local LLM: **Gemma 2 / 3 Instruct 9B**, loaded via vLLM or transformers
- RAG pipeline with embeddings + vector DB
- Document upload functionality (PDFs)
- Full OCR support for scanned PDFs (using PaddleOCR)
- Chunking + metadata extraction
- Embedding model: **BGE-M3** (multilingual)
- Vector DB: **Chroma or FAISS**
- Optional fine-tuning via **QLoRA**
- Single-user for now, architecture prepared for multi-user later
- PDFs may be large (10â€“1000 pages), multilingual, with or without text layer

The system must:

1. Allow user to upload PDFs.
2. Extract text (OCR when needed) page-by-page.
3. Chunk text with overlap.
4. Compute embeddings and store chunks in vector DB.
5. Build RAG context at query time.
6. Answer questions in **Portuguese**, but documents may be in any language.
7. Show citations (doc, page).
7. Provide a NiceGUI chat interface.
8. Provide document management UI.
9. Support fine-tuning dataset generation + QLoRA.

---

# ðŸš€ GPU CONSTRAINTS

Inference and training will happen on a GPU with **20 GB VRAM (AWS g6.xlarge)**.

Gemma 9B must run in **4-bit/8-bit mode** for inference.  
QLoRA fine-tuning must be supported.

---

# ðŸ“‚ REQUIRED PROJECT STRUCTURE

Codex must create the following folders and files (no code yet):

```
project/
  app.py
  backend/
    __init__.py
    config.py
    ocr.py
    pdf_ingestion.py
    chunker.py
    embeddings.py
    vectordb.py
    rag.py
    llm.py
  ui/
    main_ui.py
    chat_ui.py
    documents_ui.py
  finetune/
    generate_dataset.py
    train_lora.py
  data/
    pdfs/
    chunks/
    vectordb/
    adapters/
```

Each module has the following purpose:

- **config.py** â€” Global settings, model paths, embedding config, chunk sizes.
- **ocr.py** â€” PaddleOCR wrapper for detecting scanned PDFs and extracting text per page.
- **pdf_ingestion.py** â€” Streams PDFs page-by-page, applies OCR, stores raw text, sends chunks to DB.
- **chunker.py** â€” Token-based chunking with metadata.
- **embeddings.py** â€” Embedding model loader (BGE-M3) + encoding functions.
- **vectordb.py** â€” Wrapper around Chroma or FAISS: insert, search, delete, filter.
- **rag.py** â€” Build retrieval pipeline + prompt construction for context.
- **llm.py** â€” Local Gemma runtime (transformers/vLLM), unified generate() function.
- **main_ui.py** â€” NiceGUI layout + state + navigation.
- **chat_ui.py** â€” Chat interface + websocket streaming of LLM responses.
- **documents_ui.py** â€” Upload UI, delete UI, ingestion progress.
- **generate_dataset.py** â€” Create synthetic Q&A using OpenAI APIs.
- **train_lora.py** â€” QLoRA fine-tuning script for Gemma 9B.
- **app.py** â€” Entry point, builds NiceGUI app with routes.

---

# ðŸŽ¯ WHAT TO OUTPUT NOW

Output ONLY:

1. The directory structure.
2. Short description of each file.
3. No Python code.

Wait for further prompts.

```

---

# ðŸ§© **PHASE 1 â€” MULTI-PROMPT SEQUENCE (A â†’ N)**  
Each block below is a **separate prompt** you will send to Codex after the previous one completes.

---

# ðŸ”· **PROMPT A â€” Create Project Structure + Empty Files**

```markdown
# PROMPT A â€” CREATE PROJECT STRUCTURE

Generate the full project directory structure with empty files exactly as defined in the Overview Prompt (PHASE 0).  
For each file, insert only:

```
# <filename>
# (empty placeholder)
```

Do NOT generate any implementation code yet.

Follow EXACTLY this tree:

project/
  app.py
  backend/
    __init__.py
    config.py
    ocr.py
    pdf_ingestion.py
    chunker.py
    embeddings.py
    vectordb.py
    rag.py
    llm.py
  ui/
    main_ui.py
    chat_ui.py
    documents_ui.py
  finetune/
    generate_dataset.py
    train_lora.py
  data/
    pdfs/
    chunks/
    vectordb/
    adapters/

Return the full file contents for all placeholders.
```

---

# ðŸ”· **PROMPT B â€” Implement OCR Module (PaddleOCR)**

```markdown
# PROMPT B â€” IMPLEMENT backend/ocr.py

Write the complete implementation for backend/ocr.py.

Requirements:
- Use PaddleOCR (multilingual).
- Detect whether a page has text; if not â†’ run OCR.
- Function: `extract_page_text(pdf_path, page_number)`  
- Function: `is_page_scanned(text)` â†’ returns True if text too short.
- Handle large PDFs efficiently.
- Include robust error handling and logging.
- Return UTF-8 cleaned text.

Only output backend/ocr.py.
```

---

# ðŸ”· **PROMPT C â€” Implement PDF Ingestion Pipeline**

```markdown
# PROMPT C â€” IMPLEMENT backend/pdf_ingestion.py

Implement:
- Streaming read of PDF pages using PyMuPDF (fitz).
- For each page:
  - Try direct text extraction.
  - If text likely from scanned page â†’ call OCR module.
  - Normalize whitespace.
  - Save clean extracted text to data/chunks or in memory.
  - Pass each pageâ€™s text to chunker module.
- Store metadata (doc_id, filename, page number).
- Push chunks to embedding + vectorDB pipeline.

Main function:
`ingest_pdf(pdf_path: str, doc_id: str) -> List[ChunkMetadata]`

No blocking load of whole PDF.
Handle documents up to 1000 pages.

Only output backend/pdf_ingestion.py.
```

---

# ðŸ”· **PROMPT D â€” Implement Chunker**

```markdown
# PROMPT D â€” IMPLEMENT backend/chunker.py

Implement:
- Token-based chunking (600 tokens with 100-token overlap).
- Use a tokenizer (HuggingFace tokenizers library).
- Chunk metadata fields:
  - chunk_id
  - doc_id
  - page
  - start_token
  - end_token
  - text
- Function: `chunk_text(doc_id, page_num, text) -> List[Chunk]`

Only output backend/chunker.py.
```

---

# ðŸ”· **PROMPT E â€” Implement Embedding Pipeline**

```markdown
# PROMPT E â€” IMPLEMENT backend/embeddings.py

Implement:
- Load multilingual BGE-M3 embedding model (sentence-transformers or HF).
- GPU acceleration if available.
- Function: `embed_text_list(texts: List[str]) -> List[List[float]]`
- Function: `embed_query(text: str)`.

All embeddings must be float32 or float16.

Only output backend/embeddings.py.
```

---

# ðŸ”· **PROMPT F â€” Implement Vector DB Wrapper**

```markdown
# PROMPT F â€” IMPLEMENT backend/vectordb.py

Implement wrapper using Chroma or FAISS.

Functions:
- `init_vector_db(path)`
- `add_chunks(chunks, embeddings)` â€” store with metadata
- `search(query_embedding, k=8, filters=None)` â€” return list of matched chunks
- `delete_doc(doc_id)`
- `list_docs()`

All metadata must be preserved.

Only output backend/vectordb.py.
```

---

# ðŸ”· **PROMPT G â€” Implement RAG Orchestration**

```markdown
# PROMPT G â€” IMPLEMENT backend/rag.py

Implement:
- Retrieval pipeline:
  - embed query
  - search vector DB
  - rank results
- Prompt builder:
  ```
  You are an assistantâ€¦
  Question:
  {query}
  Context:
  [Doc {doc_id}, page {page}]
  {chunk_text}
  ---
  ```
- Function: `build_prompt(query, retrieved_chunks)`
- Function: `retrieve_context(query, k=8)`
- Function: `answer_question(query) -> str` (calls LLM.generate)

Only output backend/rag.py.
```

---

# ðŸ”· **PROMPT H â€” Implement LLM Wrapper (Gemma + vLLM)**

```markdown
# PROMPT H â€” IMPLEMENT backend/llm.py

Implement:
- Load Gemma 2/3 Instruct 9B in 4-bit mode
- Option 1: transformers
- Option 2: vLLM server (HTTP client)
- Unified function:
  `generate(prompt: str, max_tokens=512, temperature=0.2, stream=False)`
- If stream=True â†’ yield tokens incrementally

Model must answer in Portuguese.

Only output backend/llm.py.
```

---

# ðŸ”· **PROMPT I â€” Implement Chat UI**

```markdown
# PROMPT I â€” IMPLEMENT ui/chat_ui.py

Requirements:
- Use NiceGUI chat components.
- Display user messages and assistant responses.
- Stream LLM responses token-by-token via websocket.
- Show document citations below each answer.

Functions:
- `chat_page()`
- Handler for sending a query to RAG pipeline.

Only output ui/chat_ui.py.
```

---

# ðŸ”· **PROMPT J â€” Implement Document Management UI**

```markdown
# PROMPT J â€” IMPLEMENT ui/documents_ui.py

Implement:
- Upload control.
- Show list of documents with metadata.
- Delete document.
- Trigger ingestion pipeline.
- Show progress (page X/Y).

Functions:
- `documents_page()`

Only output ui/documents_ui.py.
```

---

# ðŸ”· **PROMPT K â€” Implement Main App + Routing**

```markdown
# PROMPT K â€” IMPLEMENT ui/main_ui.py and app.py

Implement:
- Main layout with sidebar:
  - Chat
  - Documents
- Routing
- State (current theme, user session)
- Launch NiceGUI app

Only output ui/main_ui.py and app.py.
```

---

# ðŸ”· **PROMPT L â€” Synthetic Dataset Generator (OpenAI API)**

```markdown
# PROMPT L â€” IMPLEMENT finetune/generate_dataset.py

Implement:
- Read chunked text
- Ask OpenAI (gpt-4.1-mini) to generate Q&A pairs in Portuguese
- Save JSONL file compatible with SFT/LoRA
- Controls:
  - questions_per_chunk
  - multi-hop generation

Only output finetune/generate_dataset.py.
```

---

# ðŸ”· **PROMPT M â€” QLoRA Fine-Tuning Script**

```markdown
# PROMPT M â€” IMPLEMENT finetune/train_lora.py

Implement:
- Load Gemma 9B in 4-bit
- Apply QLoRA adapters
- Train using JSONL dataset
- Save adapter weights to data/adapters/<theme>

Only output finetune/train_lora.py.
```

---

# ðŸ”· **PROMPT N â€” Final Integration and Fix Pass**

```markdown
# PROMPT N â€” FINAL REVIEW

Task:
- Inspect all generated modules.
- Fix incorrect imports.
- Add missing __init__.py adjustments.
- Ensure path references are correct.
- Generate a final README.md with:
  - installation steps
  - how to run the app
  - how to fine-tune Gemma
  - how to add new documents

Output:
- README.md
- List of integration fixes.

Do NOT rewrite full files unless needed.
```

