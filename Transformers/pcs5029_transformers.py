# -*- coding: utf-8 -*-
"""PCS5029_Transformers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1490Tz6bWqKqqWY5mpNaRuXaOVIb66SbP
"""

TOKEN_HUGGING_FACE = "TOKEN_HERE"

!huggingface-cli login

# ## ðŸ§© Conjuntos de tarefas â€” DoSMi v2 e Expertise Map

"""# Benchmark extended"""

expertise_map = {
    "qa_factual": [
        {"question": "Quem escreveu a peÃ§a Hamlet?", "expected": "William Shakespeare"},
        {"question": "Qual Ã© o maior planeta do sistema solar?", "expected": "JÃºpiter"},
        {"question": "Em que continente fica o Egito?", "expected": "Ãfrica"},
        {"question": "Quem pintou a Mona Lisa?", "expected": "Leonardo da Vinci"},
        {"question": "Qual Ã© o idioma oficial do JapÃ£o?", "expected": "JaponÃªs"},
        {"question": "Em que paÃ­s nasceu Albert Einstein?", "expected": "Alemanha"},
        {"question": "Quantos dias hÃ¡ em um ano bissexto?", "expected": "366"},
        {"question": "Qual elemento quÃ­mico tem sÃ­mbolo O?", "expected": "OxigÃªnio"},
        {"question": "Qual Ã© a capital do CanadÃ¡?", "expected": "Ottawa"},
        {"question": "Quem descobriu o Brasil?", "expected": "Pedro Ãlvares Cabral"},
    ],
    "math_gsm8k": [
        {"question": "Se JoÃ£o tem 3 maÃ§Ã£s e compra mais 2, quantas ele tem agora?", "expected": "5"},
        {"question": "Qual Ã© o resultado de 12 Ã— 8?", "expected": "96"},
        {"question": "Um trem parte Ã s 10h e chega Ã s 13h30. Quantas horas durou a viagem?", "expected": "3.5"},
        {"question": "Qual Ã© a metade de 144?", "expected": "72"},
        {"question": "Se 5x = 20, qual Ã© o valor de x?", "expected": "4"},
        {"question": "Qual Ã© o quadrado de 15?", "expected": "225"},
        {"question": "Maria tinha 10 reais e gastou 4. Quanto sobrou?", "expected": "6"},
        {"question": "Se uma pizza tem 8 fatias e JoÃ£o come 3, quantas sobram?", "expected": "5"},
        {"question": "O triplo de 7 Ã© quanto?", "expected": "21"},
        {"question": "Quanto Ã© 10% de 250?", "expected": "25"},
    ],
    "code_mbpp": [
        {"question": "Escreva uma funÃ§Ã£o Python que soma dois nÃºmeros.", "expected": "def soma(a,b): return a+b"},
        {"question": "Escreva uma funÃ§Ã£o que verifica se um nÃºmero Ã© par.", "expected": "def is_even(n): return n%2==0"},
        {"question": "Implemente uma funÃ§Ã£o que calcula o fatorial de n.", "expected": "def fatorial(n): return 1 if n==0 else n*fatorial(n-1)"},
        {"question": "Crie uma funÃ§Ã£o que retorna o maior de dois nÃºmeros.", "expected": "def maior(a,b): return a if a>b else b"},
        {"question": "Crie uma funÃ§Ã£o que inverte uma string.", "expected": "def inverte(s): return s[::-1]"},
        {"question": "Escreva uma funÃ§Ã£o que retorna se um nÃºmero Ã© primo.", "expected": "def is_prime(n): return all(n%i for i in range(2,int(n**0.5)+1)) and n>1"},
        {"question": "Escreva uma funÃ§Ã£o que retorna o quadrado de um nÃºmero.", "expected": "def quadrado(x): return x**2"},
        {"question": "Escreva uma funÃ§Ã£o que calcula a mÃ©dia de uma lista.", "expected": "def media(lst): return sum(lst)/len(lst)"},
        {"question": "Crie uma funÃ§Ã£o que concatena duas strings.", "expected": "def concatena(a,b): return a+b"},
        {"question": "Escreva uma funÃ§Ã£o que conta vogais em uma string.", "expected": "def conta_vogais(s): return sum(c.lower() in 'aeiou' for c in s)"},
    ],
    "qa_fin": [
        {"question": "Qual Ã© o cÃ³digo ISIN do ITUB4?", "expected": "BRITUBACNPR1"},
        {"question": "Qual Ã© a moeda oficial do Brasil?", "expected": "real"},
        {"question": "O que significa o termo 'free float'?", "expected": "percentual de aÃ§Ãµes em livre circulaÃ§Ã£o"},
        {"question": "Qual Ã© o Ã­ndice de aÃ§Ãµes mais famoso da B3?", "expected": "Ibovespa"},
        {"question": "Qual Ã© a funÃ§Ã£o de um 'market maker'?", "expected": "fornecer liquidez ao mercado"},
        {"question": "O que Ã© o CDI?", "expected": "Certificado de DepÃ³sito InterbancÃ¡rio"},
        {"question": "O que Ã© uma aÃ§Ã£o preferencial?", "expected": "aÃ§Ã£o com preferÃªncia em dividendos"},
        {"question": "Qual Ã© a funÃ§Ã£o do Banco Central do Brasil?", "expected": "controlar a polÃ­tica monetÃ¡ria"},
        {"question": "O que Ã© um fundo de investimento?", "expected": "veÃ­culo coletivo de aplicaÃ§Ã£o financeira"},
        {"question": "Qual Ã© a sigla da moeda americana?", "expected": "USD"},
    ],
}

# Microtarefas â€” DoSMi v2
datasets_dosmi = {
    "qa_curta": [
        {"question": "Quem escreveu Dom Quixote?", "expected": "Miguel de Cervantes"},
        {"question": "Qual Ã© a capital da FranÃ§a?", "expected": "Paris"},
        {"question": "Quem pintou Guernica?", "expected": "Pablo Picasso"},
        {"question": "Qual Ã© o idioma oficial da China?", "expected": "ChinÃªs"},
        {"question": "Quem descobriu a AmÃ©rica?", "expected": "CristÃ³vÃ£o Colombo"},
    ],
    "calculo_rapido": [
        {"question": "Quanto Ã© 23 + 57?", "expected": "80"},
        {"question": "Quanto Ã© 15 Ã— 6?", "expected": "90"},
        {"question": "Se tenho 20 e gasto 5, quanto resta?", "expected": "15"},
        {"question": "Quanto Ã© 10% de 300?", "expected": "30"},
        {"question": "Quanto Ã© 100 Ã· 4?", "expected": "25"},
    ],
    "codigo_trivial": [
        {"question": "Escreva uma funÃ§Ã£o Python que retorna o quadrado de x.", "expected": "def quadrado(x): return x**2"},
        {"question": "Crie uma funÃ§Ã£o que soma dois nÃºmeros.", "expected": "def soma(a,b): return a+b"},
        {"question": "Escreva uma funÃ§Ã£o que verifica se um nÃºmero Ã© par.", "expected": "def is_even(n): return n%2==0"},
        {"question": "Crie uma funÃ§Ã£o que inverte uma string.", "expected": "def inverte(s): return s[::-1]"},
        {"question": "Escreva uma funÃ§Ã£o que conta vogais em uma string.", "expected": "def conta_vogais(s): return sum(c.lower() in 'aeiou' for c in s)"},
    ],
    "needle_test": [
        {"question": "No texto '123 token 999', qual nÃºmero vem apÃ³s a palavra token?", "expected": "999"},
        {"question": "No texto 'A B C D token Z', qual letra vem apÃ³s token?", "expected": "Z"},
        {"question": "Encontre o nÃºmero escondido: 'abc token 42 xyz'", "expected": "42"},
        {"question": "Qual nÃºmero aparece depois de token: 'foo token 17 bar'?", "expected": "17"},
        {"question": "Qual palavra aparece apÃ³s token: 'um token dois'?", "expected": "dois"},
    ],
}


# # ðŸ§  Benchmark â€” Modelos GPT (MoE vs Densos)
# AvaliaÃ§Ã£o de throughput, latÃªncia e acurÃ¡cia em micro e macrotarefas
# Autor: Renan de Luca Avila â€” PCS5029 / EPUSP


import torch, gc, time, csv
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from pathlib import Path

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
OUT_DIR = Path("results"); OUT_DIR.mkdir(exist_ok=True)


# ## âš™ï¸ ConfiguraÃ§Ã£o de modelos e quantizaÃ§Ã£o NF4


MODELS = {
    "MoE": [
        "deepseek-ai/DeepSeek-Coder-6.7B-base",
        "mistralai/Mixtral-8x7B-Instruct-v0.1",
    ],
    "Dense": [
        "google/gemma-7b-it",
        "openchat/openchat-3.5-0106",
    ],
}

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

MAX_NEW_TOKENS = 128

# ## ðŸ”§ FunÃ§Ãµes auxiliares

# %%
def family_from_name(name: str) -> str:
    n = name.lower()
    if any(k in n for k in ["mixtral", "deepseek", "moe"]):
        return "MoE"
    return "Dense"

def compute_correct(pred, exp):
    if not pred or not exp:
        return 0.0
    p, e = pred.strip().lower(), exp.strip().lower()
    return float(e in p or p in e)

def clear_gpu():
    gc.collect()
    torch.cuda.empty_cache()

def timed_generation(model, tokenizer, prompt, max_new_tokens=128):
    t0 = time.time()
    with torch.no_grad():
        inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
        gen = model.generate(**inputs, max_new_tokens=max_new_tokens)
        output = tokenizer.decode(gen[0], skip_special_tokens=True)
    latency = time.time() - t0
    n_out = len(tokenizer(output)["input_ids"])
    tokens_per_s = n_out / latency if latency > 0 else 0
    return output, latency, tokens_per_s


# ## ðŸš€ FunÃ§Ã£o genÃ©rica de benchmark


def run_benchmark(model_name, dataset_dict, csv_path):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
    )
    family = family_from_name(model_name)

    with open(csv_path, "a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=[
            "model","family","task","question","expected","answer","correct","latency","tokens_s"
        ])
        if f.tell() == 0:
            writer.writeheader()

        for task_name, items in dataset_dict.items():
            for item in tqdm(items, desc=f"{model_name} â€” {task_name}"):
                q, exp = item["question"], item["expected"]
                try:
                    out, lat, tps = timed_generation(model, tokenizer, q, MAX_NEW_TOKENS)
                    correct = compute_correct(out, exp)
                except Exception as e:
                    out, lat, tps, correct = str(e), float("nan"), float("nan"), 0
                writer.writerow({
                    "model": model_name,
                    "family": family,
                    "task": task_name,
                    "question": q,
                    "expected": exp,
                    "answer": out,
                    "correct": correct,
                    "latency": lat,
                    "tokens_s": tps,
                })
    del model, tokenizer
    clear_gpu()


# ## ðŸ§  ExecuÃ§Ã£o â€” DoSMi v2 e Expertise Map

# %%
CSV_DOSMI = OUT_DIR / "benchmark_dosmi_v2.csv"
CSV_EXP = OUT_DIR / "benchmark_expertise.csv"

for fam, model_list in MODELS.items():
    for model_name in model_list:
        print(f"===== Rodando {model_name} â€” DoSMi v2 =====")
        run_benchmark(model_name, datasets_dosmi, CSV_DOSMI)
        print(f"===== Rodando {model_name} â€” Expertise Map =====")
        run_benchmark(model_name, expertise_map, CSV_EXP)

print(f"Benchmarks concluÃ­dos! Arquivos salvos em:\n  - {CSV_DOSMI}\n  - {CSV_EXP}")